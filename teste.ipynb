{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666e9a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ðŸ“˜ Experimento comparativo: One-hot vs Hashing vs Embeddings vs MLT\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from sklearn.metrics import accuracy_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095d75b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ratings: user id | item id | rating | timestamp\n",
    "ratings = pd.read_csv(\"ml-20m/ratings.csv\")\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Reindexar usuÃ¡rios\n",
    "user_encoder = {u: i for i, u in enumerate(ratings[\"userId\"].unique())}\n",
    "ratings[\"userId\"] = ratings[\"userId\"].map(user_encoder)\n",
    "movie_encoder = {m: i for i, m in enumerate(ratings[\"movieId\"].unique())}\n",
    "ratings[\"movieId\"] = ratings[\"movieId\"].map(movie_encoder)\n",
    "\n",
    "\n",
    "ratings[\"label\"] = (ratings[\"rating\"] >= 4).astype(int)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = ratings[[\"userId\", \"movieId\"]].values   # shape (100000, 2)\n",
    "y = ratings[\"label\"].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "display(ratings.head())\n",
    "display(ratings.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0bc30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d96227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 2. FunÃ§Ãµes utilitÃ¡rias\n",
    "# ---------------------------\n",
    "\n",
    "def build_model(input_dim, hidden_dim=64):\n",
    "    \"\"\"Rede neural simples para classificaÃ§Ã£o binÃ¡ria\"\"\"\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=(input_dim,)),\n",
    "        layers.Dense(hidden_dim, activation=\"relu\"),\n",
    "        layers.Dense(32, activation=\"relu\"),\n",
    "        layers.Dense(1, activation=\"sigmoid\")\n",
    "    ])\n",
    "    model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4adc991",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. One-Hot Encoding (MovieLens: user + movie)\n",
    "X_train_df = pd.DataFrame(X_train, columns=[\"user\", \"movie\"])\n",
    "X_test_df  = pd.DataFrame(X_test, columns=[\"user\", \"movie\"])\n",
    "\n",
    "# ---------------------------\n",
    "# 3. One-Hot Encoding (MovieLens: user + movie)\n",
    "# ---------------------------\n",
    "print(\"ðŸ”¹ One-hot encoding\")\n",
    "\n",
    "try:\n",
    "    ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True)  # sklearn >= 1.2\n",
    "except TypeError:\n",
    "    ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse=True)         # sklearn < 1.2\n",
    "\n",
    "X_train_ohe = ohe.fit_transform(X_train_df.astype(str))\n",
    "X_test_ohe  = ohe.transform(X_test_df.astype(str))\n",
    "\n",
    "# Modelo\n",
    "model = build_model(X_train_ohe.shape[1])\n",
    "\n",
    "start = time.time()\n",
    "history = model.fit(X_train_ohe, y_train, epochs=3, batch_size=1024, verbose=1)\n",
    "train_time = time.time() - start\n",
    "\n",
    "start = time.time()\n",
    "y_pred = (model.predict(X_test_ohe, verbose=0) > 0.5).astype(int)\n",
    "infer_time = (time.time() - start) / len(X_test) * 1e6\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "one_hot_results = {\n",
    "    \"DimensÃ£o\": X_train_ohe.shape[1],\n",
    "    \"ReversÃ­vel\": \"Sim\",\n",
    "    \"ParÃ¢metros aprendidos\": 0,\n",
    "    \"Tempo treino (s/Ã©poca)\": round(train_time/3,2),\n",
    "    \"Tempo inferÃªncia (Âµs)\": round(infer_time,2),\n",
    "    \"AcurÃ¡cia (%)\": round(acc*100,2)\n",
    "}\n",
    "\n",
    "print(\"âœ… Resultados One-hot:\", one_hot_results)\n",
    "\n",
    "\n",
    "#âœ… Resultados One-hot: {'DimensÃ£o': 164320, 'ReversÃ­vel': 'Sim', 'ParÃ¢metros aprendidos': 0, 'Tempo treino (s/Ã©poca)': 5029.9, 'Tempo inferÃªncia (Âµs)': 427.03, 'AcurÃ¡cia (%)': 74.18}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9db99fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Hashing Trick (MovieLens: user + movie)\n",
    "# ---------------------------\n",
    "print(\"ðŸ”¹ Hashing Trick\")\n",
    "\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "\n",
    "fh = FeatureHasher(n_features=512, input_type=\"string\")\n",
    "\n",
    "# Converter pares (user, movie) em listas de strings\n",
    "X_train_hash = fh.transform([[f\"user_{u}\", f\"movie_{m}\"] for u, m in X_train])\n",
    "X_test_hash  = fh.transform([[f\"user_{u}\", f\"movie_{m}\"] for u, m in X_test])\n",
    "\n",
    "# Modelo\n",
    "model = build_model(X_train_hash.shape[1])\n",
    "\n",
    "start = time.time()\n",
    "history = model.fit(X_train_hash, y_train, epochs=3, batch_size=1024, verbose=1)\n",
    "train_time = time.time() - start\n",
    "\n",
    "start = time.time()\n",
    "y_pred = (model.predict(X_test_hash, verbose=0) > 0.5).astype(int)\n",
    "infer_time = (time.time() - start) / len(X_test) * 1e6\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "hashing_results = {\n",
    "    \"DimensÃ£o\": X_train_hash.shape[1],\n",
    "    \"ReversÃ­vel\": \"NÃ£o\",\n",
    "    \"ParÃ¢metros aprendidos\": 0,\n",
    "    \"Tempo treino (s/Ã©poca)\": round(train_time/3,2),\n",
    "    \"Tempo inferÃªncia (Âµs)\": round(infer_time,2),\n",
    "    \"AcurÃ¡cia (%)\": round(acc*100,2)\n",
    "}\n",
    "\n",
    "print(\"âœ… Resultados Hashing:\", hashing_results)\n",
    "\n",
    "\n",
    "#âœ… Resultados Hashing: {'DimensÃ£o': 512, 'ReversÃ­vel': 'NÃ£o', 'ParÃ¢metros aprendidos': 0, 'Tempo treino (s/Ã©poca)': 144.27, 'Tempo inferÃªncia (Âµs)': 86.74, 'AcurÃ¡cia (%)': 57.87}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed86d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Embeddings Aprendidos (MovieLens: user + movie)\n",
    "# ---------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"ðŸ”¹ Embeddings\")\n",
    "embedding_dim = 32\n",
    "\n",
    "n_users = ratings[\"userId\"].nunique()\n",
    "n_movies = ratings[\"movieId\"].nunique()\n",
    "\n",
    "# Inputs separados\n",
    "user_in = layers.Input(shape=(1,), name=\"user\")\n",
    "movie_in = layers.Input(shape=(1,), name=\"movie\")\n",
    "\n",
    "# Embeddings\n",
    "user_emb = layers.Embedding(input_dim=n_users+1, output_dim=embedding_dim, name=\"user_emb\")(user_in)\n",
    "movie_emb = layers.Embedding(input_dim=n_movies+1, output_dim=embedding_dim, name=\"movie_emb\")(movie_in)\n",
    "\n",
    "# Flatten\n",
    "user_emb = layers.Flatten()(user_emb)\n",
    "movie_emb = layers.Flatten()(movie_emb)\n",
    "\n",
    "# Concatenate\n",
    "x = layers.Concatenate()([user_emb, movie_emb])\n",
    "x = layers.Dense(64, activation=\"relu\")(x)\n",
    "x = layers.Dense(32, activation=\"relu\")(x)\n",
    "out = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "model = models.Model(inputs=[user_in, movie_in], outputs=out)\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Treinar\n",
    "start = time.time()\n",
    "history = model.fit(\n",
    "    {\"user\": X_train[:,0], \"movie\": X_train[:,1]},\n",
    "    y_train,\n",
    "    epochs=3, batch_size=1024, verbose=1\n",
    ")\n",
    "train_time = time.time() - start\n",
    "\n",
    "# InferÃªncia\n",
    "start = time.time()\n",
    "y_pred = (model.predict({\"user\": X_test[:,0], \"movie\": X_test[:,1]}, verbose=0) > 0.5).astype(int)\n",
    "infer_time = (time.time() - start) / len(X_test) * 1e6\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "embedding_results = {\n",
    "    \"DimensÃ£o\": embedding_dim,\n",
    "    \"ReversÃ­vel\": \"NÃ£o\",\n",
    "    \"ParÃ¢metros aprendidos\": (n_users+1)*embedding_dim + (n_movies+1)*embedding_dim,\n",
    "    \"Tempo treino (s/Ã©poca)\": round(train_time/3,2),\n",
    "    \"Tempo inferÃªncia (Âµs)\": round(infer_time,2),\n",
    "    \"AcurÃ¡cia (%)\": round(acc*100,2)\n",
    "}\n",
    "\n",
    "print(\"âœ… Resultados Embeddings:\", embedding_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a8a746",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from numpy.linalg import LinAlgError\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# ---------------------------\n",
    "# MLT otimizado (usa preallocaÃ§Ã£o)\n",
    "# ---------------------------\n",
    "def random_invertible_matrix_mod_p(n, p, max_tries=1000):\n",
    "    for _ in range(max_tries):\n",
    "        A = np.random.randint(0, p, size=(n, n), dtype=np.int16)\n",
    "        if np.gcd(int(round(np.linalg.det(A))) % p, p) == 1:\n",
    "            return A % p\n",
    "    raise LinAlgError(f\"NÃ£o foi possÃ­vel gerar matriz invertÃ­vel em {max_tries} tentativas.\")\n",
    "\n",
    "def mlt_encode(ids, p=47, n=4, A=None):\n",
    "    ids = np.asarray(ids, dtype=np.int64)\n",
    "    if A is None:\n",
    "        A = random_invertible_matrix_mod_p(n, p)\n",
    "\n",
    "    print(f\"ðŸ”¹ Codificando {len(ids)} IDs com base {p} e dimensÃ£o {n}...\")\n",
    "    codes = np.empty((len(ids), n), dtype=np.int16)\n",
    "    for i, id_val in enumerate(ids):\n",
    "        digits = np.empty(n, dtype=np.int16)\n",
    "        x = id_val\n",
    "        for j in range(n):\n",
    "            digits[j] = x % p\n",
    "            x //= p\n",
    "        codes[i] = (A.dot(digits) % p).astype(np.int16)\n",
    "        # ðŸ”¹ loga progresso em %\n",
    "        if (i + 1) % max(1, len(ids)//10) == 0:  # a cada 10%\n",
    "            pct = (i + 1) / len(ids) * 100\n",
    "            print(f\"   Progresso: {pct:.1f}% ({i+1}/{len(ids)})\")\n",
    "    print(f\"âœ… Finalizado: shape={codes.shape}\")\n",
    "    return codes, A\n",
    "\n",
    "# ---------------------------\n",
    "# CodificaÃ§Ã£o user+movie\n",
    "# ---------------------------\n",
    "print(\"\\nðŸ”¹ Iniciando codificaÃ§Ã£o de usuÃ¡rios e filmes\")\n",
    "X_train_user, A_user = mlt_encode(X_train[:,0], p=47, n=64)\n",
    "X_test_user, _       = mlt_encode(X_test[:,0], p=47, n=64, A=A_user)\n",
    "\n",
    "X_train_movie, A_movie = mlt_encode(X_train[:,1], p=47, n=64)\n",
    "X_test_movie, _        = mlt_encode(X_test[:,1], p=47, n=64, A=A_movie)\n",
    "\n",
    "X_train_mlt = np.hstack((X_train_user, X_train_movie))\n",
    "X_test_mlt  = np.hstack((X_test_user, X_test_movie))\n",
    "print(f\"ðŸ”¹ Shape X_train_mlt={X_train_mlt.shape}, X_test_mlt={X_test_mlt.shape}\")\n",
    "\n",
    "# ðŸ”¹ NormalizaÃ§Ã£o em streaming (menos RAM)\n",
    "print(\"\\nðŸ”¹ Normalizando features...\")\n",
    "scaler = StandardScaler(copy=False)\n",
    "X_train_mlt = scaler.fit_transform(X_train_mlt)\n",
    "X_test_mlt  = scaler.transform(X_test_mlt)\n",
    "print(\"âœ… NormalizaÃ§Ã£o concluÃ­da\")\n",
    "\n",
    "# ---------------------------\n",
    "# Criar tf.data.Dataset\n",
    "# ---------------------------\n",
    "batch_size = 2048\n",
    "print(\"\\nðŸ”¹ Construindo datasets...\")\n",
    "train_dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices((X_train_mlt.astype(np.float32), y_train.astype(np.float32)))\n",
    "    .shuffle(buffer_size=len(X_train_mlt))\n",
    "    .batch(batch_size)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "test_dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices((X_test_mlt.astype(np.float32), y_test.astype(np.float32)))\n",
    "    .batch(batch_size)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "print(\"âœ… Datasets prontos\")\n",
    "\n",
    "# ---------------------------\n",
    "# Treino com dataset\n",
    "# ---------------------------\n",
    "print(\"\\nðŸ”¹ Iniciando treino do modelo...\")\n",
    "model = build_model(X_train_mlt.shape[1])\n",
    "epochs = 10\n",
    "\n",
    "start = time.time()\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = []\n",
    "    batch_count = 0\n",
    "    for X_batch, y_batch in train_dataset:\n",
    "        loss, acc = model.train_on_batch(X_batch, y_batch)\n",
    "        epoch_loss.append(loss)\n",
    "        batch_count += 1\n",
    "        if batch_count % 50 == 0:\n",
    "            print(f\"   [Ã‰poca {epoch+1}] Batch {batch_count} â†’ Loss={loss:.4f}, Acc={acc:.4f}\")\n",
    "\n",
    "    print(f\"ðŸ“Œ Ã‰poca {epoch+1}/{epochs} - Loss mÃ©dio: {np.mean(epoch_loss):.4f} ({batch_count} batches)\")\n",
    "train_time = time.time() - start\n",
    "print(\"âœ… Treino finalizado\")\n",
    "\n",
    "# ---------------------------\n",
    "# InferÃªncia\n",
    "# ---------------------------\n",
    "print(\"\\nðŸ”¹ Rodando inferÃªncia...\")\n",
    "start = time.time()\n",
    "y_pred = []\n",
    "batch_count = 0\n",
    "for X_batch, _ in test_dataset:\n",
    "    batch_pred = (model.predict_on_batch(X_batch) > 0.5).astype(np.int8)\n",
    "    y_pred.append(batch_pred)\n",
    "    batch_count += 1\n",
    "    if batch_count % 20 == 0:\n",
    "        print(f\"   Batch {batch_count} de inferÃªncia processado\")\n",
    "y_pred = np.vstack(y_pred).ravel()\n",
    "\n",
    "infer_time = (time.time() - start) / len(X_test) * 1e6\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"âœ… InferÃªncia concluÃ­da\")\n",
    "\n",
    "# ---------------------------\n",
    "# Resultados finais\n",
    "# ---------------------------\n",
    "mlt_results = {\n",
    "    \"DimensÃ£o\": X_train_mlt.shape[1],\n",
    "    \"ReversÃ­vel\": \"Sim\",\n",
    "    \"ParÃ¢metros aprendidos\": 0,\n",
    "    \"Tempo treino (s/Ã©poca)\": round(train_time/epochs, 2),\n",
    "    \"Tempo inferÃªncia (Âµs)\": round(infer_time, 2),\n",
    "    \"AcurÃ¡cia (%)\": round(acc*100, 2)\n",
    "}\n",
    "\n",
    "print(\"\\nðŸŽ¯ Resultados finais MLT:\")\n",
    "for k, v in mlt_results.items():\n",
    "    print(f\"   {k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d484d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import pyarrow.parquet as pq\n",
    "from numpy.linalg import LinAlgError\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# ---------------------------\n",
    "# MLT otimizado\n",
    "# ---------------------------\n",
    "def random_invertible_matrix_mod_p(n, p, max_tries=1000):\n",
    "    for _ in range(max_tries):\n",
    "        A = np.random.randint(0, p, size=(n, n), dtype=np.int16)\n",
    "        if np.gcd(int(round(np.linalg.det(A))) % p, p) == 1:\n",
    "            return A % p\n",
    "    raise LinAlgError(f\"NÃ£o foi possÃ­vel gerar matriz invertÃ­vel em {max_tries} tentativas.\")\n",
    "\n",
    "def mlt_encode(ids, p=47, n=4, A=None, verbose=True):\n",
    "    ids = np.asarray(ids, dtype=np.int64)\n",
    "    if A is None:\n",
    "        A = random_invertible_matrix_mod_p(n, p)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"ðŸ”¹ Codificando {len(ids)} IDs com base {p} e dimensÃ£o {n}...\")\n",
    "\n",
    "    digits = np.empty((len(ids), n), dtype=np.int16)\n",
    "    x = ids.copy()\n",
    "    for j in range(n):\n",
    "        digits[:, j] = x % p\n",
    "        x //= p\n",
    "\n",
    "    codes = (digits @ A.T) % p\n",
    "    codes = codes.astype(np.int16)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"âœ… Finalizado: shape={codes.shape}\")\n",
    "\n",
    "    return codes, A\n",
    "\n",
    "# ---------------------------\n",
    "# CodificaÃ§Ã£o user+movie\n",
    "# ---------------------------\n",
    "print(\"\\nðŸ”¹ Iniciando codificaÃ§Ã£o de usuÃ¡rios e filmes\")\n",
    "X_train_user, A_user = mlt_encode(X_train[:, 0], p=7, n=128)\n",
    "X_test_user, _       = mlt_encode(X_test[:, 0], p=7, n=128, A=A_user)\n",
    "\n",
    "X_train_movie, A_movie = mlt_encode(X_train[:, 1], p=7, n=128)\n",
    "X_test_movie, _        = mlt_encode(X_test[:, 1], p=7, n=128, A=A_movie)\n",
    "\n",
    "X_train_mlt = np.hstack((X_train_user, X_train_movie))\n",
    "X_test_mlt  = np.hstack((X_test_user, X_test_movie))\n",
    "print(f\"ðŸ”¹ Shape X_train_mlt={X_train_mlt.shape}, X_test_mlt={X_test_mlt.shape}\")\n",
    "\n",
    "# ---------------------------\n",
    "# NormalizaÃ§Ã£o em chunks (menos RAM)\n",
    "# ---------------------------\n",
    "print(\"\\nðŸ”¹ Normalizando features...\")\n",
    "scaler = StandardScaler()\n",
    "chunk_size = 1000\n",
    "\n",
    "# Ajusta mÃ©dia/desvio padrÃ£o\n",
    "for start in range(0, len(X_train_mlt), chunk_size):\n",
    "    scaler.partial_fit(X_train_mlt[start:start + chunk_size])\n",
    "\n",
    "# Aplica normalizaÃ§Ã£o inplace\n",
    "for start in range(0, len(X_train_mlt), chunk_size):\n",
    "    X_train_mlt[start:start + chunk_size] = scaler.transform(X_train_mlt[start:start + chunk_size])\n",
    "\n",
    "for start in range(0, len(X_test_mlt), chunk_size):\n",
    "    X_test_mlt[start:start + chunk_size] = scaler.transform(X_test_mlt[start:start + chunk_size])\n",
    "\n",
    "print(\"âœ… NormalizaÃ§Ã£o concluÃ­da\")\n",
    "\n",
    "# ---------------------------\n",
    "# Salvar os Parquet\n",
    "# ---------------------------\n",
    "print(\"\\nðŸ”¹ Salvando arquivos Parquet...\")\n",
    "df_train = pd.DataFrame(X_train_mlt, columns=[f\"f{i}\" for i in range(X_train_mlt.shape[1])])\n",
    "df_train[\"label\"] = y_train\n",
    "df_train.to_parquet(\"train.parquet\", index=False)\n",
    "\n",
    "df_test = pd.DataFrame(X_test_mlt, columns=[f\"f{i}\" for i in range(X_test_mlt.shape[1])])\n",
    "df_test[\"label\"] = y_test\n",
    "df_test.to_parquet(\"test.parquet\", index=False)\n",
    "print(\"âœ… Arquivos train.parquet e test.parquet salvos.\")\n",
    "\n",
    "# ---------------------------\n",
    "# FunÃ§Ãµes auxiliares para Parquet â†’ Dataset\n",
    "# ---------------------------\n",
    "def parquet_generator(parquet_file, feature_cols, label_col, batch_size=512):\n",
    "    table = pq.ParquetFile(parquet_file)\n",
    "    for batch in table.iter_batches(batch_size=batch_size):\n",
    "        df = batch.to_pandas()\n",
    "        X = df[feature_cols].to_numpy(dtype=np.float32)\n",
    "        y = df[label_col].to_numpy(dtype=np.float32)\n",
    "        yield X, y\n",
    "\n",
    "def make_dataset(parquet_file, feature_cols, label_col, batch_size=512, shuffle=False):\n",
    "    output_signature = (\n",
    "        tf.TensorSpec(shape=(None, len(feature_cols)), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(None,), dtype=tf.float32),\n",
    "    )\n",
    "    ds = tf.data.Dataset.from_generator(\n",
    "        lambda: parquet_generator(parquet_file, feature_cols, label_col, batch_size),\n",
    "        output_signature=output_signature\n",
    "    )\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=10_000)\n",
    "    return ds.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# ---------------------------\n",
    "# Criar tf.data.Dataset\n",
    "# ---------------------------\n",
    "batch_size = 512\n",
    "print(\"\\nðŸ”¹ Construindo datasets a partir de Parquet...\")\n",
    "\n",
    "schema = pq.read_schema(\"train.parquet\")\n",
    "all_cols = schema.names\n",
    "label_col = \"label\"\n",
    "feature_cols = [c for c in all_cols if c != label_col]\n",
    "num_features = len(feature_cols)\n",
    "\n",
    "train_dataset = make_dataset(\"train.parquet\", feature_cols, label_col, batch_size=batch_size, shuffle=True)\n",
    "test_dataset  = make_dataset(\"test.parquet\", feature_cols, label_col, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"âœ… Datasets prontos (streaming de Parquet) - {num_features} features detectadas\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd4c63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------------\n",
    "# Treino com dataset\n",
    "# ---------------------------\n",
    "print(\"\\nðŸ”¹ Iniciando treino do modelo...\")\n",
    "model = build_model(num_features)\n",
    "epochs = 10\n",
    "\n",
    "start = time.time()\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = []\n",
    "    batch_count = 0\n",
    "    for X_batch, y_batch in train_dataset:\n",
    "        loss, acc = model.train_on_batch(X_batch, y_batch)\n",
    "        epoch_loss.append(loss)\n",
    "        batch_count += 1\n",
    "        if batch_count % 50 == 0:\n",
    "            print(f\"   [Ã‰poca {epoch+1}] Batch {batch_count} â†’ Loss={loss:.4f}, Acc={acc:.4f}\")\n",
    "\n",
    "    print(f\"ðŸ“Œ Ã‰poca {epoch+1}/{epochs} - Loss mÃ©dio: {np.mean(epoch_loss):.4f} ({batch_count} batches)\")\n",
    "\n",
    "train_time = time.time() - start\n",
    "print(\"âœ… Treino finalizado\")\n",
    "\n",
    "# ---------------------------\n",
    "# InferÃªncia\n",
    "# ---------------------------\n",
    "print(\"\\nðŸ”¹ Rodando inferÃªncia...\")\n",
    "start = time.time()\n",
    "y_pred = []\n",
    "batch_count = 0\n",
    "for X_batch, _ in test_dataset:\n",
    "    batch_pred = (model.predict_on_batch(X_batch) > 0.5).astype(np.int8)\n",
    "    y_pred.append(batch_pred)\n",
    "    batch_count += 1\n",
    "    if batch_count % 20 == 0:\n",
    "        print(f\"   Batch {batch_count} de inferÃªncia processado\")\n",
    "y_pred = np.vstack(y_pred).ravel()\n",
    "\n",
    "infer_time = (time.time() - start) / len(X_test) * 1e6\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"âœ… InferÃªncia concluÃ­da\")\n",
    "\n",
    "# ---------------------------\n",
    "# Resultados finais\n",
    "# ---------------------------\n",
    "mlt_results = {\n",
    "    \"DimensÃ£o\": num_features,\n",
    "    \"ReversÃ­vel\": \"Sim\",\n",
    "    \"ParÃ¢metros aprendidos\": 0,\n",
    "    \"Tempo treino (s/Ã©poca)\": round(train_time/epochs, 2),\n",
    "    \"Tempo inferÃªncia (Âµs)\": round(infer_time, 2),\n",
    "    \"AcurÃ¡cia (%)\": round(acc*100, 2)\n",
    "}\n",
    "\n",
    "print(\"\\nðŸŽ¯ Resultados finais MLT:\")\n",
    "for k, v in mlt_results.items():\n",
    "    print(f\"   {k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c565fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. MLT + Autoencoder \n",
    "print(\"ðŸ”¹ MLT + Autoencoder\")\n",
    "\n",
    "p = 47\n",
    "n = 64\n",
    "bottleneck_dim = 16\n",
    "\n",
    "# Codificar usuÃ¡rios e filmes separadamente\n",
    "X_train_user, A_user = mlt_encode(X_train[:,0], p, n)\n",
    "X_test_user, _       = mlt_encode(X_test[:,0], p, n, A=A_user)\n",
    "\n",
    "X_train_movie, A_movie = mlt_encode(X_train[:,1], p, n)\n",
    "X_test_movie, _        = mlt_encode(X_test[:,1], p, n, A=A_movie)\n",
    "\n",
    "# Concatenar representaÃ§Ãµes user+movie (entrada MLT crua)\n",
    "X_train_mlt = np.hstack([X_train_user, X_train_movie])\n",
    "X_test_mlt  = np.hstack([X_test_user, X_test_movie])\n",
    "\n",
    "input_dim = X_train_mlt.shape[1]\n",
    "\n",
    "# ---------------------------\n",
    "# Definir Autoencoder\n",
    "# ---------------------------\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# ---------------------------\n",
    "# Autoencoder com Embedding no input (MLT tokens)\n",
    "# ---------------------------\n",
    "inp = layers.Input(shape=(input_dim,), dtype=\"int32\", name=\"mlt_input\")\n",
    "\n",
    "# ðŸ”¹ Embedding: cada valor inteiro (0..p-1) vai para um vetor contÃ­nuo\n",
    "emb = layers.Embedding(input_dim=p, output_dim=16, name=\"mlt_embedding\")(inp)  \n",
    "# Agora shape = (batch, input_dim, 16)\n",
    "\n",
    "# ðŸ”¹ Achatar para (batch, input_dim * 16)\n",
    "emb = layers.Flatten()(emb)\n",
    "\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "def residual_block(x, units, dropout_rate=0.2, name=\"res_block\"):\n",
    "    \"\"\"Bloco residual simples com Dense + BatchNorm + Dropout.\"\"\"\n",
    "    shortcut = x\n",
    "    x = layers.Dense(units, activation=\"relu\",\n",
    "                     kernel_regularizer=regularizers.l2(1e-4),\n",
    "                     name=f\"{name}_dense\")(x)\n",
    "    x = layers.BatchNormalization(name=f\"{name}_bn\")(x)\n",
    "    x = layers.Dropout(dropout_rate, name=f\"{name}_drop\")(x)\n",
    "    # ajustar dimensÃ£o do atalho se necessÃ¡rio\n",
    "    if shortcut.shape[-1] != units:\n",
    "        shortcut = layers.Dense(units, activation=None, name=f\"{name}_proj\")(shortcut)\n",
    "    x = layers.Add(name=f\"{name}_add\")([x, shortcut])\n",
    "    x = layers.Activation(\"relu\", name=f\"{name}_out\")(x)\n",
    "    return x\n",
    "\n",
    "# ---------------------------\n",
    "# Encoder\n",
    "# ---------------------------\n",
    "x = residual_block(emb, 128, dropout_rate=0.3, name=\"enc_res1\")\n",
    "x = residual_block(x, 32, dropout_rate=0.2, name=\"enc_res2\")\n",
    "bottleneck = layers.Dense(bottleneck_dim, activation=\"linear\", name=\"bottleneck\")(x)\n",
    "\n",
    "# ---------------------------\n",
    "# Decoder\n",
    "# ---------------------------\n",
    "x_dec = residual_block(bottleneck, 32, dropout_rate=0.2, name=\"dec_res1\")\n",
    "x_dec = residual_block(x_dec, 128, dropout_rate=0.3, name=\"dec_res2\")\n",
    "out_recon = layers.Dense(input_dim, activation=\"linear\", name=\"reconstruction\")(x_dec)\n",
    "\n",
    "# ---------------------------\n",
    "# Modelos\n",
    "# ---------------------------\n",
    "autoenc = models.Model(inputs=inp, outputs=out_recon, name=\"MLT_Autoencoder\")\n",
    "\n",
    "\n",
    "autoenc.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "\n",
    "\n",
    "# Treinar Autoencoder (nÃ£o supervisionado)\n",
    "autoenc.fit(\n",
    "    X_train_mlt, X_train_mlt,\n",
    "    epochs=10, batch_size=1024, verbose=1,\n",
    "    validation_split=0.1\n",
    ")\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Extrair encoder treinado\n",
    "# ---------------------------\n",
    "encoder = models.Model(\n",
    "    inputs=autoenc.input,\n",
    "    outputs=autoenc.get_layer(\"bottleneck\").output,\n",
    "    name=\"MLT_Encoder\"\n",
    ")\n",
    "\n",
    "# ---------------------------\n",
    "# Gerar embeddings comprimidos (64D) para treino do modelo padrÃ£o\n",
    "# ---------------------------\n",
    "X_train_emb = encoder.predict(X_train_mlt, verbose=0)\n",
    "X_test_emb  = encoder.predict(X_test_mlt, verbose=0)\n",
    "\n",
    "# ---------------------------\n",
    "# Usar o modelo padrÃ£o (build_model) com bottleneck como entrada\n",
    "# ---------------------------\n",
    "model = build_model(bottleneck_dim)\n",
    "\n",
    "start = time.time()\n",
    "history = model.fit(X_train_emb, y_train, epochs=10, batch_size=1024, verbose=1)\n",
    "train_time = time.time() - start\n",
    "\n",
    "# InferÃªncia\n",
    "start = time.time()\n",
    "y_pred = (model.predict(X_test_emb, verbose=0) > 0.5).astype(int)\n",
    "infer_time = (time.time() - start) / len(X_test) * 1e6\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "mlt_autoenc_results = {\n",
    "    \"DimensÃ£o\": bottleneck_dim,\n",
    "    \"ReversÃ­vel\": \"Sim (MLT+Decoder)\",\n",
    "    \"ParÃ¢metros aprendidos\": autoenc.count_params(),\n",
    "    \"Tempo treino (s/Ã©poca)\": round(train_time/100, 2),\n",
    "    \"Tempo inferÃªncia (Âµs)\": round(infer_time, 2),\n",
    "    \"AcurÃ¡cia (%)\": round(acc*100, 2)\n",
    "}\n",
    "\n",
    "print(\"âœ… Resultados MLT + Autoencoder:\", mlt_autoenc_results)\n",
    "#âœ… Resultados MLT + Autoencoder: {'DimensÃ£o': 16, 'ReversÃ­vel': 'Sim (MLT+Decoder)', 'ParÃ¢metros aprendidos': 79758, 'Tempo treino (s/Ã©poca)': 2.13, 'Tempo inferÃªncia (Âµs)': 14.35, 'AcurÃ¡cia (%)': 62.53}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd0eb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 7. ComparaÃ§Ã£o Final\n",
    "# ---------------------------\n",
    "\n",
    "# ðŸ“Œ RepresentaÃ§Ãµes fixas (justa comparaÃ§Ã£o)\n",
    "results_fixed = pd.DataFrame(\n",
    "    [one_hot_results, hashing_results, mlt_results, mlt_autoenc_results],\n",
    "    index=[\"One-hot\", \"Hashing\", \"MLT\", \"MLT+Autoencoder\"]\n",
    ")\n",
    "\n",
    "# ðŸ“Œ Embeddings supervisionados (baseline separado)\n",
    "results_embed = pd.DataFrame(\n",
    "    [embedding_results],\n",
    "    index=[\"Embeddings (supervisionados)\"]\n",
    ")\n",
    "\n",
    "print(\"\\nðŸ“Š Resultados Comparativos â€“ CodificaÃ§Ãµes Fixas\")\n",
    "display(results_fixed.sort_values(\"AcurÃ¡cia (%)\", ascending=False))\n",
    "\n",
    "print(\"\\nðŸ“Š Resultado â€“ Embeddings Supervisionados (baseline separado)\")\n",
    "display(results_embed)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".mlt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
